{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse, io\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR='C:\\\\Users\\\\RISHABH\\\\Documents\\\\input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatrain = pd.read_csv(os.path.join(RAW_DATA_DIR,'gender_age_train.csv'),\n",
    "                      )\n",
    "gatest = pd.read_csv(os.path.join(RAW_DATA_DIR,'gender_age_test.csv'),\n",
    "                     )\n",
    "phone = pd.read_csv(os.path.join(RAW_DATA_DIR,'phone_brand_device_model.csv'))\n",
    "# removing duplicate values in phone dataframe so doesn't create problems while joining dataframes\n",
    "phone=phone.drop_duplicates('device_id',keep='first')\n",
    "\n",
    "events = pd.read_csv(path.join(RAW_DATA_DIR, 'events.csv'),\n",
    "                     parse_dates=['timestamp'],\n",
    "                     infer_datetime_format=True,\n",
    "                     )\n",
    "\n",
    "appevents = pd.read_csv(path.join(RAW_DATA_DIR, 'app_events.csv'),\n",
    "                        dtype={'is_installed':bool, 'is_active':bool})\n",
    "\n",
    "applabels = pd.read_csv(os.path.join(RAW_DATA_DIR, 'app_labels.csv')) \n",
    "folds_5=pd.read_csv(os.path.join(RAW_DATA_DIR, 'folds_5.csv')) \n",
    "folds_10=pd.read_csv(os.path.join(RAW_DATA_DIR,'folds_10.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "Xtrain=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtrain_all.npz'))  \n",
    "Xtest=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtest_all.npz')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = gatrain['group']\n",
    "label_group = LabelEncoder()\n",
    "ytrain = label_group.fit_transform(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  8.8min finished\n",
      "C:\\Users\\RISHABH\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('classifier' , LogisticRegression())]) \n",
    "\n",
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : [ 'l2'],\n",
    "    'classifier__C' : np.logspace(-3, 1, 10),\n",
    "    'classifier__solver' : ['lbfgs']},\n",
    "]\n",
    "\n",
    "# Create grid search object\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, scoring='neg_log_loss', cv = 5, verbose=True, n_jobs=-1) \n",
    "best_clf=clf.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(C=0.021544346900318832, class_weight=None, dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'classifier__C': 0.021544346900318832,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.2881861356011615"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Predicitng on best model Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RISHABH\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr=LogisticRegression(C=0.02154,penalty='l2',solver='lbfgs').fit(Xtrain,ytrain) \n",
    "\n",
    "p_group=np.zeros((Xtest.shape[0],12)) \n",
    "p_group=lr.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab=LabelEncoder()   \n",
    "\n",
    "# group is target class in train set converting to labels\n",
    "lab.fit(gatrain['group'].astype(str)) \n",
    "\n",
    "pred_test=pd.DataFrame(p_group,index=gatest.device_id,columns=list(lab.classes_)) \n",
    "pred_test=pred_test.reset_index() \n",
    "\n",
    "pred_test.to_csv('pred_test_lr.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Documents/input/lr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning on Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:  5.2min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "param_grid = [\n",
    "    {'classifier' : [DecisionTreeClassifier()],\n",
    "    'classifier__min_samples_split' : list(range(2,10,2)),\n",
    "    'classifier__max_depth' : list(range(6,22,5))}\n",
    "]\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, scoring='neg_log_loss', cv = 5, verbose=True, n_jobs=-1) \n",
    "best_clf=clf.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=6,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " 'classifier__max_depth': 6,\n",
       " 'classifier__min_samples_split': 6}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.4332835621797053"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on best model Dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr=DecisionTreeClassifier(max_depth= 6,min_samples_split= 6).fit(Xtrain,ytrain) \n",
    "\n",
    "p_group=np.zeros((Xtest.shape[0],12)) \n",
    "p_group=lr.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test=pd.DataFrame(p_group,index=gatest.device_id,columns=list(lab.classes_)) \n",
    "pred_test=pred_test.reset_index() \n",
    "\n",
    "pred_test.to_csv('pred_test_dt.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Documents/input/dt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making different models for devices with events and devices without events as the data will be significantly different \n",
    "## 1. devices with events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=events.device_id.unique() \n",
    "#creating featura has events for train and test\n",
    "gatrain['has_events']=gatrain.device_id.apply(lambda x:1 if x in s else 0)  \n",
    "gatest['has_events']=gatest.device_id.apply(lambda x:1 if x in s else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsetting data to include only has events==1\n",
    "gatrain=gatrain[gatrain['has_events']==1] \n",
    "gatest=gatest[gatest['has_events']==1]  \n",
    "#creating row number as feild to help create sparse matrix\n",
    "gatrain['trainrow'] = np.arange(gatrain.shape[0])\n",
    "gatest['testrow'] = np.arange(gatest.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging data to acess other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging train and test to get phone details associated with phone \n",
    "gatrain=gatrain.merge(phone,on='device_id')  \n",
    "gatrain=gatrain.merge(folds_5,on='device_id')\n",
    "gatest=gatest.merge(phone,on='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab=LabelEncoder()   \n",
    "\n",
    "# group is target class in train set converting to labels\n",
    "gatrain['group']=lab.fit_transform(gatrain['group'].astype(str)) \n",
    "\n",
    "gatrain['gender']=lab.fit_transform(gatrain['gender'].astype(str)) \n",
    "'''appending phone brand in train and test to fit label encoder\n",
    "    as the test contains phone brand which are not present in train \n",
    "    so not to get data leak we have to append them'''\n",
    "lab.fit(np.append(gatrain.phone_brand.values,gatest.phone_brand.values))  \n",
    "#converting phone brand in test and  train\n",
    "gatrain['phone_brand']=lab.transform(gatrain['phone_brand']) \n",
    "gatest['phone_brand']=lab.transform(gatest['phone_brand']) \n",
    "#similar to phone brand we do for device model.\n",
    "lab.fit(np.append(gatrain['device_model'].values,gatest['device_model'].values)) \n",
    "gatrain['device_model']=lab.transform(gatrain['device_model']) \n",
    "gatest['device_model']=lab.transform(gatest['device_model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I am trying a new technique of using conditional probability in which we first predict gender and then use gender as additional feature to predict the group so the group should be converted to a range of 1 to 6 and divided based on age for both male and female."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating feature age group for applying conditional probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getg(x): \n",
    "#if gender is female return group as it is already in 1-6 range\n",
    "    if x['gender']==0: \n",
    "        return x['group']\n",
    "    else:  \n",
    "#if gender is male return group-6 to make it in 1-6 range.\n",
    "        return x['group']-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function to create new feature age_group\n",
    "gatrain['age_group']=gatrain.apply(getg,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting index to create features as below\n",
    "events=events.set_index('event_id') \n",
    "gatrain=gatrain.set_index('device_id') \n",
    "gatest=gatest.set_index('device_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App feature for device containing number of times app is opened "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting apps to labels\n",
    "appencoder = LabelEncoder().fit(appevents.app_id)\n",
    "appevents['app'] = appencoder.transform(appevents.app_id) \n",
    "# create a feature containing number of times the app is opened and finding the row in trainset or test set it belongs to.\n",
    "deviceapps = (appevents.merge(events[['device_id']], how='left',left_on='event_id',right_index=True)\n",
    "                       .groupby(['device_id','app'])['app'].agg(['size'])\n",
    "                       .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                       .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                       .reset_index())\n",
    "deviceapps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating sparse matrix for app feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "napps = len(appencoder.classes_)\n",
    "# separate train and test subset and create sparse matrixes\n",
    "d = deviceapps.dropna(subset=['trainrow'])\n",
    "Xtr_app_inst = csr_matrix( ( d['size'], (d['trainrow'], d['app']) ),\n",
    "                             shape=(gatrain.shape[0], napps)\n",
    "                          )\n",
    "\n",
    "d = deviceapps.dropna(subset=['testrow'])\n",
    "Xte_app_inst = csr_matrix( (d['size'], (d['testrow'], d['app'])),\n",
    "                            shape=(gatest.shape[0], napps)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating app labels feature containing number of times label is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applabels = applabels.loc[applabels.app_id.isin(appevents.app_id.unique())]\n",
    "applabels['app'] = appencoder.transform(applabels.app_id)\n",
    "labelencoder = LabelEncoder().fit(applabels.label_id)\n",
    "applabels['label'] = labelencoder.transform(applabels.label_id)\n",
    "nlabels = len(labelencoder.classes_)\n",
    "# create a feature containing number of times the label is used finding the row in trainset or test set it belongs to.\n",
    "devicelabels = (deviceapps[['device_id','app']]\n",
    "                .merge(applabels[['app','label']])\n",
    "                .groupby(['device_id','label'])['app'].agg(['size'])\n",
    "                .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                .reset_index())\n",
    "devicelabels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sparse matrix for label feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# separate train and test subset and create sparse matrixes\n",
    "d = devicelabels.dropna(subset=['trainrow'])\n",
    "Xtr_label_inst = csr_matrix( (d['size'], (d['trainrow'], d['label'])),\n",
    "                             shape=(gatrain.shape[0], nlabels)\n",
    "                             )\n",
    "\n",
    "d = devicelabels.dropna(subset=['testrow'])\n",
    "Xte_label_inst = csr_matrix( (d['size'], (d['testrow'], d['label'])),\n",
    "                             shape=(gatest.shape[0], nlabels)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hour feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find hour from timestamp\n",
    "events['hour'] = events.timestamp.apply(lambda x: x.hour) \n",
    "#counting number of times events occur at an hour \n",
    "events_cout_hourofday = (events.groupby(['device_id','hour'])['hour'].agg(['size'])\n",
    "                    .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                    .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                    .reset_index())  \n",
    "#creating sparse matrix of hour feature\n",
    "# separate train and test subset and create sparse matrixes\n",
    "d = events_cout_hourofday.dropna(subset=['trainrow'])\n",
    "Xtr_event_on_hourofday = csr_matrix((d['size'], (d.trainrow, d.hour)),\n",
    "                      shape=(gatrain.shape[0],d.hour.nunique()))\n",
    "\n",
    "d = events_cout_hourofday.dropna(subset=['testrow'])\n",
    "Xte_event_on_hourofday = csr_matrix((d['size'], (d.testrow, d.hour)),\n",
    "                      shape=(gatest.shape[0],d.hour.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events['week_day'] = events.timestamp.dt.weekday\n",
    "#counting number of times events occur at a day \n",
    "events_cout_weekday = (events.groupby(['device_id','week_day'])['week_day'].agg(['size'])\n",
    "                    .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                    .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                    .reset_index())\n",
    "d = events_cout_weekday.dropna(subset=['trainrow'])\n",
    "Xtr_event_on_weekday = csr_matrix((d['size'], (d.trainrow, d['week_day'])),\n",
    "                      shape=(gatrain.shape[0],d.week_day.nunique()))\n",
    "\n",
    "d = events_cout_weekday.dropna(subset=['testrow'])\n",
    "Xte_event_on_weekday = csr_matrix((d['size'], (d.testrow, d['week_day'])),\n",
    "                      shape=(gatest.shape[0],d.week_day.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## latitude and longitude feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''longitude and latitude converting (0,0) coordinate which is default coordinate if\n",
    "location data is not available to np.nan for applying nanmedian'''\n",
    "events.longitude=events.longitude.apply(lambda x:np.NaN if x==0 else x) \n",
    "events.latitude=events.latitude.apply(lambda x:np.NaN if x==0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find median latitude and longitude for a device with events.\n",
    "latitude=events.groupby('device_id')['latitude'].apply(np.nanmedian) \n",
    "longitude=events.groupby('device_id')['longitude'].apply(np.nanmedian) \n",
    "latitude=latitude.reset_index()\n",
    "longitude=longitude.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index to merge with longitude and latitude\n",
    "gatrain=gatrain.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gatest=gatest.merge(latitude,on='device_id') \n",
    "gatest=gatest.merge(longitude,on='device_id') \n",
    "te_longitude=gatest.longitude.values.reshape((-1,1)) \n",
    "te_latitude=gatest.latitude.values.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatrain=gatrain.merge(longitude,on='device_id') \n",
    "longitude=gatrain.longitude.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatrain=gatrain.merge(latitude,on='device_id') \n",
    "latitude=gatrain.latitude.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude=latitude.reshape((-1,1)) \n",
    "longitude=longitude.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=hstack((Xtr_app_inst,Xtr_label_inst,Xtr_event_on_hourofday,Xtr_event_on_weekday,longitude,latitude,gatrain.phone_brand.values.reshape((-1,1)),gatrain.device_model.values.reshape((-1,1)))) \n",
    "Xtest=hstack((Xte_app_inst,Xte_label_inst,Xte_event_on_hourofday,Xte_event_on_weekday,te_longitude,te_latitude,gatest.phone_brand.values.reshape((-1,1)),gatest.device_model.values.reshape((-1,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gender= gatrain['gender'].values.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = max(gatrain.fold)\n",
    "nbags = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'booster':'gbtree',\n",
    "    'objective':'reg:logistic',\n",
    "    'eval_metric':'logloss',\n",
    "    'learning_rate':0.025,\n",
    "    'max_depth':6,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.5,\n",
    "    'colsample_bylevel':0.5 , \n",
    "    'min_child_weight':5\n",
    "}\n",
    "gridsearch_params = [\n",
    "    (max_depth,  min_child_weight)\n",
    "    for max_depth in range(6,10)\n",
    "    for min_child_weight in range(5,8)\n",
    "] \n",
    "    \n",
    " \n",
    "i=1\n",
    "\n",
    "num_boost_round=1000\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None \n",
    "dtr = xgb.DMatrix(Xtrain.tocsr(), label = y_gender, missing = np.nan,nthread=-1)\n",
    "\n",
    "for max_depth,  min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                              min_child_weight))\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['colsample_tree'] = colsample_tree\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtr,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'logloss'},\n",
    "        early_stopping_rounds=10, \n",
    "    )\n",
    "    # Update best MAE\n",
    "    mean_logloss = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(\"\\tlogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting gender for train using k fold cross validation and bagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## xgb data \n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "p_gender = np.zeros((Xtrain.shape[0],2))  \n",
    "y_gender= gatrain['gender'].values.reshape((-1,1)) \n",
    "# Starting k fold cross validation.\n",
    "for i in range(1,nfolds+1): \n",
    "    #Dividing data in train and test based on fold\n",
    "    inTr = gatrain.index[gatrain.fold != i]\n",
    "    inTe = gatrain.index[gatrain.fold == i] \n",
    "    y_gender= gatrain['gender'].values.reshape((-1,1)) \n",
    "    dtr = xgb.DMatrix(Xtrain.tocsr()[inTr,:], label = y_gender[inTr], missing = np.nan)\n",
    "    dcv= xgb.DMatrix(Xtrain.tocsr()[inTe,:],label = y_gender[inTe], missing= np.nan) \n",
    "    p=np.zeros(len(inTe))\n",
    "## parameter set\n",
    "    param = {'booster':'gbtree',\n",
    "             'objective':'reg:logistic',\n",
    "             'eval_metric':'logloss',\n",
    "             'learning_rate':0.025,\n",
    "             'max_depth':8,\n",
    "             'subsample':0.8,\n",
    "             'colsample_bytree':0.5,\n",
    "             'colsample_bylevel':0.5,\n",
    "             'min_child_weight':5} \n",
    "    for j in range(nbags):\n",
    "        print('Iter', i, '- gender\\n')\n",
    "  \n",
    "  ## train model\n",
    "        bst_gender = xgb.train(param,\n",
    "                         dtr,\n",
    "                         932)\n",
    "  \n",
    "  ## prediction\n",
    "        p =p+ bst_gender.predict(dcv)\n",
    "        \n",
    "        del bst_gender \n",
    "    p=p/nbags\n",
    "    prob = np.c_[1-p, p] \n",
    "    p_gender[inTe,] = prob\n",
    "    score = log_loss(y_gender[inTe], prob)\n",
    "    print('Gender fold', i, '- Score', round(score,6), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for repeating values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_rep(x, reps=1, each=False, length=0):\n",
    "    \"\"\" implementation of functionality of rep() and rep_len() from R\n",
    "\n",
    "    Attributes:\n",
    "        x: numpy array, which will be flattened\n",
    "        reps: int, number of times x should be repeated\n",
    "        each: logical; should each element be repeated reps times before the next\n",
    "        length: int, length desired; if >0, overrides reps argument\n",
    "    \"\"\"\n",
    "    if length > 0:\n",
    "        reps = np.int(np.ceil(length / x.size))\n",
    "    x = np.repeat(x, reps)\n",
    "    if(not each):\n",
    "        x = x.reshape(-1, reps).T.ravel() \n",
    "    if length > 0:\n",
    "        x = x[0:length]\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''we want to repeat each row twice because we are adding gender as additional variable which takes two values 0 and 1  \n",
    "so we are repeating the indices twice'''\n",
    "idx = np_rep(np.arange(Xtest.shape[0]), each = True,reps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeating each row twice\n",
    "xtest_mod=Xtest.tocsr()[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeating gender values till Xtest\n",
    "g=np_rep(np.arange(2),reps=Xtest.shape[0]) \n",
    "g=g.reshape((-1,1)) \n",
    "g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_mod=hstack((g,xtest_mod)) \n",
    "dtest=xgb.DMatrix(xtest_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict group with gender as aditional feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(666) \n",
    "y_age_group=gatrain.age_group.values.reshape((-1,1)) \n",
    "p_age_group=np.zeros((gatrain.shape[0],12)) \n",
    "for i in range(1,nfolds+1): \n",
    "    #ddividing in train and test\n",
    "    inTr=gatrain.index[gatrain.fold!=i] \n",
    "    inTe=gatrain.index[gatrain.fold==i]\n",
    "    #for val add extra feature gender\n",
    "    idx = np_rep(np.arange(len(inTe)), each = True,reps=2)\n",
    "    val=Xtrain.tocsr()[inTe,:] \n",
    "    val=val[idx,:] \n",
    "    g=np_rep(np.arange(2),reps=len(inTe)) \n",
    "    g=g.reshape((-1,1)) \n",
    "    xtest_mod=hstack((g,val)) \n",
    "    #adding extra feature gender for train\n",
    "    dtr = xgb.DMatrix(hstack((y_gender[inTr], Xtrain.tocsr()[inTr,:])), label = y_age_group[inTr], missing = np.nan,nthread=-1) \n",
    "    dte=xgb.DMatrix(xtest_mod,missing=np.nan)\n",
    "    param = {'booster':'gbtree',\n",
    "             'objective':'multi:softprob',\n",
    "             'eval_metric':'mlogloss',\n",
    "             'num_class':6,\n",
    "             'learning_rate':0.025,\n",
    "             'max_depth':8,\n",
    "             'subsample':0.8,\n",
    "             'colsample_bytree':0.5,\n",
    "             'colsample_bylevel':0.5,\n",
    "             'min_child_weight':5}\n",
    "\n",
    "    p=np.zeros((len(inTe),12))\n",
    "\n",
    "    for j in range(nbags):\n",
    "        print('Iter', i, '- gender\\n')\n",
    "  \n",
    "          ## train model\n",
    "        bst_age_group = xgb.train(param,\n",
    "                         dtr,\n",
    "                         932)\n",
    "  \n",
    "  ## prediction\n",
    "        p = p + bst_age_group.predict(dte).reshape(-1,12)\n",
    "  \n",
    "        del bst_age_group\n",
    "\n",
    "\n",
    "    p = p/nbags \n",
    "    \n",
    "    p_age_group[inTe] = p \n",
    "    \n",
    "    print('Age_group fold', i, '- Score', round(score,6), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction using definition of conditional probability for events feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_group = np.concatenate((np.multiply(np.divide(p_age_group[...,:6] ,p_age_group[...,0:6].sum(axis=1).reshape(-1,1)), p_gender[...,0].reshape(-1,1)),\n",
    "                np.multiply(np.divide(p_age_group[...,6:12] ,p_age_group[...,6:12].sum(axis=1).reshape(-1,1)) ,p_gender[...,1].reshape(-1,1))),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatrain = pd.read_csv(os.path.join(RAW_DATA_DIR,'gender_age_train.csv'),\n",
    "                      ) \n",
    "lab=LabelEncoder()   \n",
    "\n",
    "# group is target class in train set converting to labels\n",
    "lab.fit(gatrain['group'].astype(str)) \n",
    "gatrain['has_events']=gatrain.device_id.apply(lambda x:1 if x in s else 0)  \n",
    "gatrain=gatrain[gatrain['has_events']==1] \n",
    "pred_train_events=pd.DataFrame(p_group,index=gatrain.device_id,columns=list(lab.classes_)) \n",
    "pred_train_events=pred_train_events.reset_index() \n",
    "pred_train_events.head() \n",
    "pred_train_events.to_csv('pred_train_events.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. for devices without events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatrain = pd.read_csv(os.path.join(RAW_DATA_DIR,'gender_age_train.csv'),\n",
    "                      )\n",
    "gatest = pd.read_csv(os.path.join(RAW_DATA_DIR,'gender_age_test.csv'),\n",
    "                     )\n",
    "phone = pd.read_csv(os.path.join(RAW_DATA_DIR,'phone_brand_device_model.csv'))\n",
    "# add rownumber = encoding of device_id\n",
    "phone=phone.drop_duplicates('device_id',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatrain=gatrain.merge(phone,on='device_id') \n",
    "gatest=gatest.merge(phone,on='device_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label encoding variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab=LabelEncoder()  \n",
    "gatrain['group']=lab.fit_transform(gatrain['group'].astype(str)) \n",
    "\n",
    "gatrain['gender']=lab.fit_transform(gatrain['gender'].astype(str)) \n",
    " \n",
    "lab.fit(np.append(gatrain.phone_brand.values,gatest.phone_brand.values))  \n",
    "gatrain['phone_brand']=lab.transform(gatrain['phone_brand']) \n",
    "gatest['phone_brand']=lab.transform(gatest['phone_brand']) \n",
    "lab.fit(np.append(gatrain['device_model'].values,gatest['device_model'].values)) \n",
    "gatrain['device_model']=lab.transform(gatrain['device_model']) \n",
    "gatest['device_model']=lab.transform(gatest['device_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatrain['has_events']=gatrain.device_id.apply(lambda x:1 if x in s else 0)  \n",
    "gatest['has_events']=gatest.device_id.apply(lambda x:1 if x in s else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test for noevents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=np.concatenate((gatrain.phone_brand.values.reshape(-1,1),gatrain.device_model.values.reshape(-1,1)),axis=1) \n",
    "Xtest=np.concatenate((gatest.phone_brand.values.reshape(-1,1),gatest.device_model.values.reshape(-1,1)),axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for xgboost for noevents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'booster':'gbtree',\n",
    "    'objective':'reg:logistic',\n",
    "    'eval_metric':'logloss',\n",
    "    'learning_rate':0.025,\n",
    "    'max_depth':6,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.5,\n",
    "    'colsample_bylevel':0.5 , \n",
    "    'min_child_weight':5\n",
    "}\n",
    "gridsearch_params = [\n",
    "    (max_depth,  min_child_weight)\n",
    "    for max_depth in range(6,10)\n",
    "    for min_child_weight in range(5,8)\n",
    "] \n",
    "    \n",
    " \n",
    "i=1\n",
    "\n",
    "num_boost_round=1000\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None \n",
    "dtr = xgb.DMatrix(Xtrain, label = y_gender, missing = np.nan,nthread=-1)\n",
    "\n",
    "for max_depth,  min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                              min_child_weight))\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['colsample_tree'] = colsample_tree\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtr,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'logloss'},\n",
    "        early_stopping_rounds=10, \n",
    "    )\n",
    "    # Update best MAE\n",
    "    mean_logloss = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(\"\\tlogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbags = 5\n",
    "\n",
    "y_gender= gatrain['gender'].values.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatrain=gatrain.merge(folds_5,on='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_gender=np.zeros((len(gatrain.index),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting gender using k fold cross validation for no events data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## xgb data \n",
    "import xgboost as xgb \n",
    "for i in range(1,nfolds+1): \n",
    "     \n",
    "    inTr = gatrain.index[gatrain.fold != i]\n",
    "    inTe = gatrain.index[(gatrain.fold == i) & (gatrain.has_events==0) ] \n",
    "    y_gender= gatrain['gender'].values.reshape((-1,1)) \n",
    "    dtr = xgb.DMatrix(Xtrain[inTr,:], label = y_gender[inTr], missing = np.nan)\n",
    "    dcv= xgb.DMatrix(Xtrain[inTe,:],label = y_gender[inTe], missing= np.nan) \n",
    "    p=np.zeros(len(inTe))\n",
    "## parameter set\n",
    "    param = {'booster':'gbtree',\n",
    "             'objective':'reg:logistic',\n",
    "             'eval_metric':'logloss',\n",
    "             'learning_rate':0.025,\n",
    "             'max_depth':9,\n",
    "             'subsample':0.8,\n",
    "             'colsample_bytree':0.5,\n",
    "             'colsample_bylevel':0.5,\n",
    "             'min_child_weight':5} \n",
    "    for j in range(nbags):\n",
    "        print('Iter', i, '- gender\\n')\n",
    "  \n",
    "  ## train model\n",
    "        bst_gender = xgb.train(param,\n",
    "                         dtr,\n",
    "                         1000)\n",
    "  \n",
    "  ## prediction\n",
    "        p =p+ bst_gender.predict(dcv)\n",
    "        \n",
    "        del bst_gender \n",
    "    p=p/nbags\n",
    "    prob = np.c_[1-p, p] \n",
    "    p_gender[inTe,] = prob\n",
    "    score = log_loss(y_gender[inTe], prob)\n",
    "    print('Gender fold', i, '- Score', round(score,6), '\\n')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_gender=p_gender[gatrain.index[gatrain.has_events==0],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding gender as additional for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "Xtest=Xtest.tocsr()\n",
    "'''we want to repeat each row twice because we are adding gender as additional variable which takes two values 0 and 1  \n",
    "so we are repeating the indices twice'''\n",
    "idx = np_rep(np.arange(Xtest.shape[0]), each = True,reps=2) \n",
    "#repeating each row twice \n",
    "xtest_mod=Xtest.tocsr()[idx,:]\n",
    "#repeating gender values till Xtest\n",
    "g=np_rep(np.arange(2),reps=Xtest.shape[0]) \n",
    "g=g.reshape((-1,1)) \n",
    "xtest_mod=hstack((g,xtest_mod)) \n",
    "dtest=xgb.DMatrix(xtest_mod) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting age group using gender as additional feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(666) \n",
    "y_age_group=gatrain.age_group.values.reshape((-1,1)) \n",
    "p_age_group=np.zeros((Xtrain.shape[0],12)) \n",
    "for i in range(1,nfolds+1): \n",
    "    inTr=gatrain.index[gatrain.fold!=i] \n",
    "    inTe=gatrain.index[(gatrain.fold==i) & (gatrain.has_events==0)] \n",
    "    idx = np_rep(np.arange(len(inTe)), each = True,reps=2)\n",
    "    val=Xtrain[inTe,:] \n",
    "    val=val[idx,:] \n",
    "    g=np_rep(np.arange(2),reps=len(inTe)) \n",
    "    g=g.reshape((-1,1)) \n",
    "    xtest_mod=np.concatenate((g,val),axis=1) \n",
    "    dtr = xgb.DMatrix(np.concatenate((y_gender[inTr], Xtrain[inTr,:]),axis=1), label = y_age_group[inTr], missing = np.nan) \n",
    "    dte=xgb.DMatrix(xtest_mod,missing=np.nan)\n",
    "    param = {'booster':'gbtree',\n",
    "             'objective':'multi:softprob',\n",
    "             'eval_metric':'mlogloss',\n",
    "             'num_class':6,\n",
    "             'learning_rate':0.025,\n",
    "             'max_depth':9,\n",
    "             'subsample':0.8,\n",
    "             'colsample_bytree':0.5,\n",
    "             'colsample_bylevel':0.5,\n",
    "             'min_child_weight':5}\n",
    "\n",
    "    p=np.zeros((len(inTe),12))\n",
    "\n",
    "    for j in range(nbags):\n",
    "        print('Iter', i, '- gender\\n')\n",
    "  \n",
    "          ## train model\n",
    "        bst_age_group = xgb.train(param,\n",
    "                         dtr,\n",
    "                         1060)\n",
    "  \n",
    "  ## prediction\n",
    "        p = p + bst_age_group.predict(dte).reshape(-1,12)\n",
    "  \n",
    "        del bst_age_group\n",
    "\n",
    "\n",
    "    p = p/nbags \n",
    "    \n",
    "    p_age_group[inTe] = p \n",
    "    \n",
    "    print('Age_group fold', i, '- Score', round(score,6), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction using definition of conditional probability for events feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_age_group=p_age_group[gatrain.index[gatrain.has_events==0],:]\n",
    "p_group= np.concatenate((np.multiply(np.divide(p_age_group[...,:6] ,p_age_group[...,0:6].sum(axis=1).reshape(-1,1)), p_gender[...,0].reshape(-1,1)),\n",
    "                np.multiply(np.divide(p_age_group[...,6:12] ,p_age_group[...,6:12].sum(axis=1).reshape(-1,1)) ,p_gender[...,1].reshape(-1,1))),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatrain = pd.read_csv(os.path.join(RAW_DATA_DIR,'gender_age_train.csv'),\n",
    "                      ) \n",
    "lab=LabelEncoder()   \n",
    "\n",
    "# group is target class in train set converting to labels\n",
    "lab.fit(gatrain['group'].astype(str)) \n",
    "gatrain['has_events']=gatrain.device_id.apply(lambda x:1 if x in s else 0)  \n",
    "gatrain=gatrain[gatrain['has_events']==0] \n",
    "pred_train_noevents=pd.DataFrame(p_group,index=gatrain.device_id,columns=list(lab.classes_)) \n",
    "pred_train_noevents=pred_train_noevents.reset_index() \n",
    "pred_train_noevents.head() \n",
    "pred_train_noevents.to_csv('pred_train_noevents.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test with events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Train for events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=hstack((Xtr_app_inst,Xtr_label_inst,Xtr_event_on_hourofday,Xtr_event_on_weekday,longitude,latitude,gatrain.phone_brand.values.reshape((-1,1)),gatrain.device_model.values.reshape((-1,1)))) \n",
    "Xtest=hstack((Xte_app_inst,Xte_label_inst,Xte_event_on_hourofday,Xte_event_on_weekday,te_longitude,te_latitude,gatest.phone_brand.values.reshape((-1,1)),gatest.device_model.values.reshape((-1,1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting gender for test with events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(666)  \n",
    "nbags=10\n",
    "y_gender=gatrain.gender.values.reshape((-1,1)) \n",
    "p_gender=np.zeros((Xtest.shape[0],2))\n",
    "dtr = xgb.DMatrix( Xtrain, label = y_gender, missing = np.nan) \n",
    "dtest=xgb.DMatrix(Xtest)\n",
    "param = {'booster':'gbtree',\n",
    "             'objective':'reg:logistic',\n",
    "             'eval_metric':'logloss',\n",
    "             'learning_rate':0.025,\n",
    "             'max_depth':8,\n",
    "             'subsample':0.8,\n",
    "             'colsample_bytree':0.5,\n",
    "             'colsample_bylevel':0.5,\n",
    "              'min_child_weight':5}\n",
    "\n",
    "\n",
    "\n",
    "for i in range(nbags):\n",
    "    print('Iter', i, '- gender\\n')\n",
    "  \n",
    "  ## train model\n",
    "    bst_gender = xgb.train(param,\n",
    "                         dtr,\n",
    "                         932)\n",
    "  \n",
    "  ## prediction\n",
    "    p= bst_gender.predict(dtest)\n",
    "    p_gender = p_gender + np.c_[1-p,p]\n",
    "  \n",
    "    del bst_gender\n",
    "\n",
    "\n",
    "p_gender = p_gender/nbags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding gender as additional for test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "Xtest=Xtest.tocsr()\n",
    "'''we want to repeat each row twice because we are adding gender as additional variable which takes two values 0 and 1  \n",
    "so we are repeating the indices twice'''\n",
    "idx = np_rep(np.arange(Xtest.shape[0]), each = True,reps=2) \n",
    "#repeating each row twice \n",
    "xtest_mod=Xtest.tocsr()[idx,:]\n",
    "#repeating gender values till Xtest\n",
    "g=np_rep(np.arange(2),reps=Xtest.shape[0]) \n",
    "g=g.reshape((-1,1)) \n",
    "xtest_mod=hstack((g,xtest_mod)) \n",
    "dtest=xgb.DMatrix(xtest_mod) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predecting age_group for test events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(666) \n",
    "y_age_group=gatrain.age_group.values.reshape((-1,1)) \n",
    "p_age_group=np.zeros((Xtest.shape[0],12))\n",
    "dtr = xgb.DMatrix(hstack((y_gender, Xtrain)), label = y_age_group, missing = np.nan,nthread=-1)\n",
    "param = {'booster':'gbtree',\n",
    "             'objective':'multi:softprob',\n",
    "             'eval_metric':'mlogloss',\n",
    "             'num_class':6,\n",
    "             'learning_rate':0.025,\n",
    "             'max_depth':8,\n",
    "             'subsample':0.8,\n",
    "             'colsample_bytree':0.5,\n",
    "             'colsample_bylevel':0.5,\n",
    "              'min_child_weight':5}\n",
    "\n",
    "\n",
    "\n",
    "for i in range(nbags):\n",
    "    print('Iter', i, '- gender\\n')\n",
    "  \n",
    "  ## train model\n",
    "    bst_age_group = xgb.train(param,\n",
    "                         dtr,\n",
    "                         932)\n",
    "  \n",
    "  ## prediction\n",
    "    p_age_group = p_age_group + bst_age_group.predict(dtest).reshape(-1,12)\n",
    "  \n",
    "    del bst_age_group\n",
    "\n",
    "\n",
    "p_age_group = p_age_group/nbags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using definition of conditional probability to calculate p_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_group = np.concatenate((np.multiply(np.divide(p_age_group[...,:6] ,p_age_group[...,0:6].sum(axis=1).reshape(-1,1)), p_gender[...,0].reshape(-1,1)),\n",
    "                np.multiply(np.divide(p_age_group[...,6:12] ,p_age_group[...,6:12].sum(axis=1).reshape(-1,1)) ,p_gender[...,1].reshape(-1,1))),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatest = pd.read_csv(os.path.join(RAW_DATA_DIR,'gender_age_test.csv'),\n",
    "                      ) \n",
    "lab=LabelEncoder()   \n",
    "\n",
    "# group is target class in train set converting to labels\n",
    "lab.fit(gatrain['group'].astype(str)) \n",
    "gatest['has_events']=gatest.device_id.apply(lambda x:1 if x in s else 0)  \n",
    "gatest=gatest[gatest['has_events']==1] \n",
    "pred_test_events=pd.DataFrame(p_group,index=gatest.device_id,columns=list(lab.classes_)) \n",
    "pred_test_events=pred_test_events.reset_index() \n",
    "\n",
    "pred_test_events.to_csv('pred_test_events.csv')\n",
    "pred_test_events.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test for noevents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtrain=np.concatenate((gatrain.phone_brand.values.reshape(-1,1),gatrain.device_model.values.reshape(-1,1)),axis=1) \n",
    "Xtest=np.concatenate((gatest.phone_brand.values.reshape(-1,1),gatest.device_model.values.reshape(-1,1)),axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict gender for noevents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(666)  \n",
    "nbags=10\n",
    "y_gender=gatrain.gender.values.reshape((-1,1)) \n",
    "p_gender=np.zeros((Xtest.shape[0],2))\n",
    "dtr = xgb.DMatrix( Xtrain, label = y_gender, missing = np.nan) \n",
    "dtest=xgb.DMatrix(Xtest)\n",
    "param = {'booster':'gbtree',\n",
    "             'objective':'reg:logistic',\n",
    "             'eval_metric':'logloss',\n",
    "             'learning_rate':0.025,\n",
    "             'max_depth':9,\n",
    "             'subsample':0.8,\n",
    "             'colsample_bytree':0.5,\n",
    "             'colsample_bylevel':0.5,\n",
    "              'min_child_weight':5}\n",
    "\n",
    "\n",
    "\n",
    "for i in range(nbags):\n",
    "    print('Iter', i, '- gender\\n')\n",
    "  \n",
    "  ## train model\n",
    "    bst_gender = xgb.train(param,\n",
    "                         dtr,\n",
    "                         932)\n",
    "  \n",
    "  ## prediction\n",
    "    p= bst_gender.predict(dtest)\n",
    "    p_gender = p_gender + np.c_[1-p,p]\n",
    "  \n",
    "    del bst_gender\n",
    "\n",
    "\n",
    "p_gender = p_gender/nbags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_gender=p_gender[gatest.index[gatest.has_events==0],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding gender as additional for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "Xtest=Xtest.tocsr()\n",
    "'''we want to repeat each row twice because we are adding gender as additional variable which takes two values 0 and 1  \n",
    "so we are repeating the indices twice'''\n",
    "idx = np_rep(np.arange(Xtest.shape[0]), each = True,reps=2) \n",
    "#repeating each row twice \n",
    "xtest_mod=Xtest.tocsr()[idx,:]\n",
    "#repeating gender values till Xtest\n",
    "g=np_rep(np.arange(2),reps=Xtest.shape[0]) \n",
    "g=g.reshape((-1,1)) \n",
    "xtest_mod=hstack((g,xtest_mod)) \n",
    "dtest=xgb.DMatrix(xtest_mod) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predecting age_group for test noevents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(666) \n",
    "y_age_group=gatrain.age_group.values.reshape((-1,1)) \n",
    "p_age_group=np.zeros((Xtest.shape[0],12))\n",
    "dtr = xgb.DMatrix(np.concatenate((y_gender, Xtrain),axis=1), label = y_age_group, missing = np.nan,nthread=-1)\n",
    "param = {'booster':'gbtree',\n",
    "             'objective':'multi:softprob',\n",
    "             'eval_metric':'mlogloss',\n",
    "             'num_class':6,\n",
    "             'learning_rate':0.025,\n",
    "             'max_depth':9,\n",
    "             'subsample':0.8,\n",
    "             'colsample_bytree':0.5,\n",
    "             'colsample_bylevel':0.5,\n",
    "              'min_child_weight':5}\n",
    "\n",
    "\n",
    "\n",
    "for i in range(nbags):\n",
    "    print('Iter', i, '- gender\\n')\n",
    "  \n",
    "  ## train model\n",
    "    bst_age_group = xgb.train(param,\n",
    "                         dtr,\n",
    "                         932)\n",
    "  \n",
    "  ## prediction\n",
    "    p_age_group = p_age_group + bst_age_group.predict(dtest).reshape(-1,12)\n",
    "  \n",
    "    del bst_age_group\n",
    "\n",
    "\n",
    "p_age_group = p_age_group/nbags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using definition of conditional probability to calculate p_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_group = np.concatenate((np.multiply(np.divide(p_age_group[...,:6] ,p_age_group[...,0:6].sum(axis=1).reshape(-1,1)), p_gender[...,0].reshape(-1,1)),\n",
    "                np.multiply(np.divide(p_age_group[...,6:12] ,p_age_group[...,6:12].sum(axis=1).reshape(-1,1)) ,p_gender[...,1].reshape(-1,1))),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatest = pd.read_csv(os.path.join(RAW_DATA_DIR,'gender_age_test.csv'),\n",
    "                      ) \n",
    "lab=LabelEncoder()   \n",
    "\n",
    "# group is target class in train set converting to labels\n",
    "lab.fit(gatrain['group'].astype(str)) \n",
    "gatest['has_events']=gatest.device_id.apply(lambda x:1 if x in s else 0)  \n",
    "gatest=gatest[gatest['has_events']==0] \n",
    "pred_test_noevents=pd.DataFrame(p_group,index=gatest.device_id,columns=list(lab.classes_)) \n",
    "pred_test_noevents=pred_test_noevents.reset_index() \n",
    "\n",
    "pred_test_noevents.to_csv('pred_test_noevents.csv')\n",
    "pred_test_noevents.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_test=pd.concat([pred_test_noevents,pred_test_events]) \n",
    "xgb_sub=gatest.merge(pred_test,on='device_id') \n",
    "\n",
    "xgb_sub.to_csv('xgb_sub.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Documents/input/xgb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now trying different Keras NN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch generator for validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading train data and prediction variabl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "Xtrain=scipy.sparse.load_npz('Xtrain_all.npz') \n",
    "train=pd.read_csv('gender_age_train.csv') \n",
    "ytrain = train['group']\n",
    "label_group = LabelEncoder()\n",
    "ytrain = label_group.fit_transform(ytrain) \n",
    "folds = pd.read_csv('folds_5.csv')['fold'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=Xtrain.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=Xtrain.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return(model)\n",
    "\n",
    "## cv params\n",
    "nfolds = np.max(folds)\n",
    "nbags = 1\n",
    "\n",
    "p_group = np.zeros((Xtrain.shape[0], 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, nfolds+1):\n",
    "    ## cv index\n",
    "    inTr = [idx for idx, fold in enumerate(folds) if fold != i]\n",
    "    inTe = [idx for idx, fold in enumerate(folds) if fold == i]\n",
    "    ## train data\n",
    "    xtr = Xtrain[inTr]\n",
    "    ytr = ytrain[inTr]\n",
    "    ## validation data\n",
    "    xval = Xtrain[inTe]\n",
    "    yval = ytrain[inTe]\n",
    "    ## object to store predictions\n",
    "    pred = np.zeros((xval.shape[0], 12))\n",
    "    for j in range(nbags):\n",
    "        model = nn_model()\n",
    "        ## training\n",
    "        fit = model.fit_generator(generator = batch_generator(xtr,ytr, 400, True),\n",
    "                                  nb_epoch = 18,\n",
    "                                  samples_per_epoch = np.ceil(69984/400),\n",
    "                                  verbose = 0)\n",
    "        ## prediction\n",
    "        pred += model.predict_generator(generator = batch_generatorp(xval, 800, False), val_samples = xval.shape[0])\n",
    "    ## average predictions\n",
    "    pred /= nbags\n",
    "    p_group[inTe] = pred\n",
    "    score = log_loss(yval, pred)\n",
    "    print('Fold ', i, '-', score, '\\n')\n",
    "\n",
    "score = log_loss(Y, p_group)\n",
    "print('Total score', score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest=scipy.sparse.load_npz('Xtest_all.npz') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbags = 10\n",
    "\n",
    "p_group = np.zeros((Xtest.shape[0], 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nbags):\n",
    "    ## cv index\n",
    "    \n",
    "    ## train data\n",
    "    xtr = Xtrain\n",
    "    ytr = ytrain\n",
    "    ## validation data\n",
    "    xval = Xtest\n",
    "   \n",
    "    ## object to store predictions\n",
    "\n",
    "    model = nn_model()\n",
    "        ## training\n",
    "    fit = model.fit_generator(generator = batch_generator(xtr,ytr, 400, True),\n",
    "                                  nb_epoch = 18,\n",
    "                                  samples_per_epoch = np.ceil(69984/400),\n",
    "                                  verbose = 2)\n",
    "        ## prediction\n",
    "    p_group += model.predict_generator(generator = batch_generatorp(xval, 800, False), val_samples = xval.shape[0])\n",
    "    ## average predictions\n",
    "    \n",
    "    \n",
    "\n",
    "p_group /= nbags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('gender_age_test.csv')\n",
    "targetencoder=LabelEncoder() \n",
    "targetencoder.fit_transform(train.group)\n",
    "cols=list(targetencoder.classes_) \n",
    "cols=['device_id']+cols  \n",
    "pred=pd.DataFrame(data=p_group,index=test.device_id) \n",
    "pred=pred.reset_index() \n",
    "pred.columns=cols \n",
    "pred.to_csv('pred_keras_test_bag10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Keras model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this we are training different models for data with events and data without events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading train data and prediction variable fo events data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "Xtrain_events=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtrain_events.npz')) \n",
    "s=events.device_id.unique() \n",
    "#creating featura has events for train and test\n",
    "gatrain['has_events']=gatrain.device_id.apply(lambda x:1 if x in s else 0) \n",
    "gatrain=gatrain[gatrain['has_events']==1]\n",
    "ytrain = gatrain['group']\n",
    "label_group = LabelEncoder()\n",
    "ytrain = label_group.fit_transform(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging with fold file containing fold information\n",
    "gatrain=gatrain.merge(folds_5,on='device_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(200, input_dim = Xtrain_events.shape[1], init = 'he_normal'))\n",
    "  model.add(PReLU())\n",
    "  model.add(Dropout(0.4))\n",
    "  model.add(Dense(100, init = 'he_normal'))\n",
    "  model.add(PReLU())\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(12, init = 'he_normal', activation = 'softmax'))\n",
    "  adagrad = Adagrad(lr = 0.005, epsilon = 1e-08)\n",
    "  model.compile(loss = 'sparse_categorical_crossentropy', optimizer = adagrad, metrics = ['accuracy'])\n",
    "  return(model)\n",
    "\n",
    "## cv params\n",
    "nfolds = 5\n",
    "nbags = 5\n",
    "\n",
    "p_group = np.zeros((Xtrain_events.shape[0], 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on training set for events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, nfolds+1):\n",
    "    ## cv index\n",
    "    inTr=gatrain.index[gatrain.fold!=i] \n",
    "    inTe=gatrain.index[gatrain.fold==i]\n",
    "    ## train data\n",
    "    xtr = Xtrain_events[inTr]\n",
    "    ytr = ytrain[inTr]\n",
    "    ## validation data\n",
    "    xval = Xtrain_events[inTe]\n",
    "    yval = ytrain[inTe]\n",
    "    ## object to store predictions\n",
    "    pred = np.zeros((xval.shape[0], 12))\n",
    "    for j in range(nbags):\n",
    "        model = nn_model()\n",
    "        ## training\n",
    "        fit = model.fit_generator(generator = batch_generator(xtr,ytr, 200, True),\n",
    "                                  nb_epoch = 80,\n",
    "                                  steps_per_epoch = 5,\n",
    "                                  verbose = 0)\n",
    "        ## prediction\n",
    "        pred += model.predict_generator(generator = batch_generatorp(xval, 800, False), steps = xval.shape[0]/800)\n",
    "    ## average predictions\n",
    "    pred /= nbags\n",
    "    p_group[inTe] = pred\n",
    "    score = log_loss(yval, pred)\n",
    "    print('Fold ', i, '-', score, '\\n')\n",
    "\n",
    "score = log_loss(Y, p_group)\n",
    "print('Total score', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_events=pd.DataFrame(p_group,index=gatrain.device_id[gatrain.has_events==1]) \n",
    "\n",
    "pred_train_events.to_csv('pred_train_keras2_events.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading train data and prediction variable for noevents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtrain_all_brand_model.npz')) \n",
    "s=events.device_id.unique() \n",
    "#creating featura has events for train and test\n",
    "gatrain['has_events']=gatrain.device_id.apply(lambda x:1 if x in s else 0) \n",
    "gatrain=gatrain[gatrain['has_events']==0]\n",
    "ytrain = gatrain['group']\n",
    "label_group = LabelEncoder()\n",
    "ytrain = label_group.fit_transform(ytrain) \n",
    "p_group = np.zeros((Xtrain.shape[0], 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on training set for noevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, nfolds+1):\n",
    "    ## cv index\n",
    "    inTr=gatrain.index[gatrain.fold!=i] \n",
    "    inTe=gatrain.index[gatrain.fold==i]\n",
    "    ## train data\n",
    "    xtr = Xtrain[inTr]\n",
    "    ytr = ytrain[inTr]\n",
    "    ## validation data\n",
    "    xval = Xtrain[inTe]\n",
    "    yval = ytrain[inTe]\n",
    "    ## object to store predictions\n",
    "    pred = np.zeros((xval.shape[0], 12))\n",
    "    for j in range(nbags):\n",
    "        model = nn_model()\n",
    "        ## training\n",
    "        fit = model.fit_generator(generator = batch_generator(xtr,ytr, 400, True),\n",
    "                                  nb_epoch = 30,\n",
    "                                  steps_per_epoch = 40000/400,\n",
    "                                  verbose = 0)\n",
    "        ## prediction\n",
    "        pred += model.predict_generator(generator = batch_generatorp(xval, 800, False), steps = xval.shape[0]/800)\n",
    "    ## average predictions\n",
    "    pred /= nbags\n",
    "    p_group[inTe] = pred\n",
    "    score = log_loss(yval, pred)\n",
    "    print('Fold ', i, '-', score, '\\n')\n",
    "\n",
    "score = log_loss(ytrain, p_group)\n",
    "print('Total score', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsetting predictions which have no events in them\n",
    "p_group=p_group[gatrain.index[gatrain.has_events==0]]\n",
    "pred_train_noevents=pd.DataFrame(p_group,index=gatrain.device_id[gatrain.has_events==0]) \n",
    "\n",
    "pred_train_noevents.to_csv('pred_train_keras2_noevents.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now predicting on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading train data,test data and prediction variable for events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "Xtrain=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtrain_events.npz'))  \n",
    "Xtest=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtest_events.npz')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s=events.device_id.unique() \n",
    "#creating featura has events for train and test\n",
    "gatrain['has_events']=gatrain.device_id.apply(lambda x:1 if x in s else 0) \n",
    "gatrain=gatrain[gatrain.has_events==1]\n",
    "ytrain = gatrain['group']\n",
    "label_group = LabelEncoder()\n",
    "ytrain = label_group.fit_transform(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on testing set for events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or i in range(nbags):\n",
    "    ## cv index\n",
    "\n",
    "    ## train data\n",
    "    xtr = Xtrain\n",
    "    ytr = ytrain\n",
    "    ## validation data\n",
    "    xval = Xtest\n",
    "    \n",
    "    ## object to store predictions\n",
    "\n",
    "    \n",
    "    model = nn_model()\n",
    "        ## training\n",
    "    fit = model.fit_generator(generator = batch_generator(xtr,ytr, 200, True),\n",
    "                                  nb_epoch = 80,\n",
    "                                  samples_per_epoch = 5,\n",
    "                                  verbose = 0)\n",
    "        ## prediction\n",
    "    p_group += model.predict_generator(generator = batch_generatorp(xval, 800, False), steps = xval.shape[0]/800)\n",
    "    ## average predictions\n",
    "p_group /= nbags\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatest['has_events']=gatest.device_id.apply(lambda x:1 if x in s else 0) \n",
    "gatest=gatest[gatest.has_events==1]\n",
    "pred_test_events=pd.DataFrame(p_group,index=gatest.device_id) \n",
    "\n",
    "pred_test_events.to_csv('pred_test_keras2_events.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading train data and prediction variable for noevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "Xtrain=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtrain_all_brand_model.npz'))  \n",
    "Xtest=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtest_all_brand_model.npz')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest=Xtest.tocsr() \n",
    "Xtrain=Xtrain.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = gatrain['group']\n",
    "label_group = LabelEncoder()\n",
    "ytrain = label_group.fit_transform(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on no events data so only retaining those values\n",
    "gatest['has_events']=gatest.device_id.apply(lambda x:1 if x in s else 0) \n",
    "Xtest=Xtest[gatest.index[gatest['has_events']==0],:]\n",
    "gatest=gatest[gatest.has_events==0] \n",
    "nbags = 10\n",
    "\n",
    "p_group = np.zeros((gatest.shape[0], 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on testing set for noevents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nbags):\n",
    "    ## cv index\n",
    "\n",
    "    ## train data\n",
    "    xtr = Xtrain\n",
    "    ytr = ytrain\n",
    "    ## validation data\n",
    "    xval = Xtest\n",
    "    \n",
    "    ## object to store predictions\n",
    "\n",
    "    \n",
    "    model = nn_model()\n",
    "        ## training\n",
    "    fit = model.fit_generator(generator = batch_generator(xtr,ytr, 200, True),\n",
    "                                  nb_epoch = 80,\n",
    "                                  samples_per_epoch = 5,\n",
    "                                  verbose = 0)\n",
    "        ## prediction\n",
    "    p_group += model.predict_generator(generator = batch_generatorp(xval, 800, False), steps = xval.shape[0]/800)\n",
    "    ## average predictions\n",
    "p_group /= nbags\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_noevents=pd.DataFrame(p_group,index=gatest.device_id) \n",
    "\n",
    "pred_test_noevents.to_csv('pred_test_keras2_noevents.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Documents/input/keras2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading train data,test data and prediction variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "Xtrain=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtrain_all.npz'))  \n",
    "Xtest=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtest_all.npz')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = gatrain['group']\n",
    "label_group = LabelEncoder()\n",
    "ytrain = label_group.fit_transform(ytrain)  \n",
    "gatrain=gatrain.merge(folds_5,on='device_id')\n",
    "nfolds = np.max(folds_5)\n",
    "nbags = 5 \n",
    "p_group = np.zeros((Xtrain.shape[0], 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, nfolds+1):\n",
    "    ## cv index\n",
    "    inTr=gatrain.index[gatrain.fold!=i] \n",
    "    inTe=gatrain.index[gatrain.fold==i]\n",
    "    ## train data\n",
    "    xtr = Xtrain[inTr]\n",
    "    ytr = ytrain[inTr]\n",
    "    ## validation data\n",
    "    xval = Xtrain[inTe]\n",
    "    yval = ytrain[inTe]\n",
    "    ## object to store predictions\n",
    "    pred = np.zeros((xval.shape[0], 12))\n",
    "    for j in range(nbags):\n",
    "        model = nn_model()\n",
    "        ## training\n",
    "        fit = model.fit_generator(generator = batch_generator(xtr,ytr, 200, True),\n",
    "                                  nb_epoch = 200,\n",
    "                                  steps_per_epoch = 5,\n",
    "                                  verbose = 0)\n",
    "        ## prediction\n",
    "        pred += model.predict_generator(generator = batch_generatorp(xval, 800, False), steps = xval.shape[0]/800)\n",
    "    ## average predictions\n",
    "    pred /= nbags\n",
    "    p_group[inTe] = pred\n",
    "    score = log_loss(yval, pred)\n",
    "    print('Fold ', i, '-', score, '\\n')\n",
    "\n",
    "score = log_loss(ytrain, p_group)\n",
    "print('Total score', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_keras3=pd.DataFrame(p_group,index=gatrain.device_id) \n",
    "\n",
    "pred_train_keras3.to_csv('pred_train_keras3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_group=np.zeros((Xtest.shape[0],12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbags=10\n",
    "for i in range(nbags):\n",
    "    ## cv index\n",
    "\n",
    "    ## train data\n",
    "    xtr = Xtrain\n",
    "    ytr = ytrain\n",
    "    ## validation data\n",
    "    xval = Xtest\n",
    "    \n",
    "    ## object to store predictions\n",
    "\n",
    "    \n",
    "    model = nn_model()\n",
    "        ## training\n",
    "    fit = model.fit_generator(generator = batch_generator(xtr,ytr, 200, True),\n",
    "                                  nb_epoch = 200,\n",
    "                                  samples_per_epoch = 5,\n",
    "                                  verbose = 0)\n",
    "        ## prediction\n",
    "    p_group += model.predict_generator(generator = batch_generatorp(xval, 800, False), steps = xval.shape[0]/800)\n",
    "    ## average predictions\n",
    "p_group /= nbags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_keras3=pd.DataFrame(p_group,index=gatest.device_id) \n",
    "\n",
    "pred_test_keras3.to_csv('pred_test_keras3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Documents/input/keras3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4th Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading train data,test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "Xtrain=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtrain_all.npz'))  \n",
    "Xtest=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtest_all.npz')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New feature Model frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_freq = phone[\"device_model\"].value_counts().to_frame()\n",
    "mf_encoder = LabelEncoder().fit(model_freq.device_model) \n",
    "# creating new feature model_frequency\n",
    "model_freq['model_freq']=mf_encoder.transform(model_freq['device_model'])\n",
    "\n",
    "# merging with train and test\n",
    "gatrain=gatrain.merge(model_freq, how='left', left_on=\"device_model\", right_index=True)\n",
    "gatest=gatest.merge(model_freq, how='left', left_on=\"device_model\", right_index=True)\n",
    "gatest[\"model_freq\"]=gatest[\"model_freq\"].fillna(1) # fill not found frequencies with 1\n",
    "#creating sparse matrix for the model frequency\n",
    "\n",
    "Xtr_model_freq = csr_matrix((np.ones(gatrain.shape[0]),\n",
    "                       (gatrain.trainrow, gatrain[\"model_freq\"])))\n",
    "Xte_model_freq = csr_matrix((np.ones(gatest.shape[0]),\n",
    "                       (gatest.testrow, gatest[\"model_freq\"])))\n",
    "\n",
    "print('Model frequency features: train shape {}, test shape {}'.format(Xtr_model_freq.shape, Xte_model_freq.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New feature brand frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_freq = phone[\"phone_brand\"].value_counts().to_frame()\n",
    "bf_encoder = LabelEncoder().fit(brand_freq.phone_brand)\n",
    "# creating new feature brand_frequency\n",
    "brand_freq['brand_freq']=bf_encoder.transform(brand_freq['phone_brand'])\n",
    "\n",
    "# merging with train and test\n",
    "gatrain=gatrain.merge(brand_freq, how='left', left_on=\"phone_brand\", right_index=True)\n",
    "gatest=gatest.merge(brand_freq, how='left', left_on=\"phone_brand\", right_index=True)\n",
    "gatest[\"brand_freq\"]=gatest[\"brand_freq\"].fillna(1) # fill not found frequencies with 1\n",
    "#creating sparse matrix for the brand frequency\n",
    "Xtr_brand_freq = csr_matrix((np.ones(gatrain.shape[0]),\n",
    "                       (gatrain.trainrow, gatrain.brand_freq)))\n",
    "\n",
    "Xte_brand_freq = csr_matrix((np.ones(gatest.shape[0]),\n",
    "                       (gatest.testrow, gatest.brand_freq)))\n",
    "\n",
    "print('Brand frequency features: train shape {}, test shape {}'.format(Xtr_brand_freq.shape, Xte_brand_freq.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of events feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating number of events feature and scaling it to 0 to 1 range \n",
    "events_cout = (events.groupby('device_id')['timestamp'].agg(['size'])\n",
    "                    .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                    .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                    .reset_index())\n",
    "events_cout.size = (np.log((events_cout['size'])))\n",
    "events_cout.size = events_cout.size/events_cout.size.max()\n",
    "#sparse matrix of events feature\n",
    "d = events_cout.dropna(subset=['trainrow'])\n",
    "Xtr_eventsize = csr_matrix((d.iloc[:,1], (d.trainrow, np.zeros(d.shape[0]))),\n",
    "                      shape=(gatrain.shape[0],1))\n",
    "\n",
    "d = events_cout.dropna(subset=['testrow'])\n",
    "Xte_eventsize = csr_matrix((d.iloc[:,1], (d.testrow, np.zeros(d.shape[0]))),\n",
    "                      shape=(gatest.shape[0],1))\n",
    "print('Labels data: train shape {}, test shape {}'.format(Xtr_eventsize.shape, Xte_eventsize.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events=events.set_index('event_id')\n",
    "gatrain=gatrain.set_index('device_id') \n",
    "gatest=gatest.set_index('device_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hour feature bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #find hour from timestamp \n",
    "events['hour'] = events.timestamp.apply(lambda x: x.hour) \n",
    "#counting number of times events occur at an hour\n",
    "events_cout_hourofday = (events.groupby(['device_id','hour'])['hour'].agg(['size'])\n",
    "                    .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                    .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                    .reset_index())\n",
    "d = events_cout_hourofday.dropna(subset=['trainrow']) \n",
    "#creating sparse matrix of hour feature\n",
    "# separate train and test subset and create sparse matrixes\n",
    "Xtr_event_on_hourofday = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.hour)),\n",
    "                      shape=(gatrain.shape[0],d.hour.nunique()))\n",
    "\n",
    "d = events_cout_hourofday.dropna(subset=['testrow'])\n",
    "Xte_event_on_hourofday = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.hour)),\n",
    "                      shape=(gatest.shape[0],d.hour.nunique()))\n",
    "print('Labels data: train shape {}, test shape {}'.format(Xtr_event_on_hourofday.shape, Xte_event_on_hourofday.shape))\n",
    "# #find hour from timestamp \n",
    "events['hour'] = events.timestamp.apply(lambda x: x.hour) \n",
    "#counting number of times events occur at an hour\n",
    "events_cout_hourofday = (events.groupby(['device_id','hour'])['hour'].agg(['size'])\n",
    "                    .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                    .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                    .reset_index())\n",
    "d = events_cout_hourofday.dropna(subset=['trainrow']) \n",
    "#creating sparse matrix of hour feature\n",
    "# separate train and test subset and create sparse matrixes\n",
    "Xtr_event_on_hourofday = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.hour)),\n",
    "                      shape=(gatrain.shape[0],d.hour.nunique()))\n",
    "\n",
    "d = events_cout_hourofday.dropna(subset=['testrow'])\n",
    "Xte_event_on_hourofday = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.hour)),\n",
    "                      shape=(gatest.shape[0],d.hour.nunique()))\n",
    "print('Labels data: train shape {}, test shape {}'.format(Xtr_event_on_hourofday.shape, Xte_event_on_hourofday.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_lab = applabels.groupby(\"app_id\")[\"label_id\"].apply(\n",
    "    lambda x: \" \".join(str(s) for s in x))\n",
    "\n",
    "#joining all applabels together and creating a feature\n",
    "print(\"# Read App Events\") \n",
    "appevents['app_lab'] = np.nan\n",
    "appevents[\"app_lab\"] = appevents[\"app_id\"].map(app_lab)\n",
    "appevents = appevents.groupby(\"event_id\")[\"app_lab\"].apply(\n",
    "    lambda x: \" \".join(str(s) for s in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events=events.reset_index()\n",
    "events['app_lab']=np.nan\n",
    "events[\"app_lab\"] = events[\"event_id\"].map(appevents) \n",
    "#all app labels are joined together for one device\n",
    "events = events.groupby(\"device_id\")[\"app_lab\"].apply(\n",
    "    lambda x: \" \".join(str(s) for s in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash_data(train, test):\n",
    "    df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "    split_len = len(train)\n",
    "\n",
    "    # TF-IDF Feature\n",
    "    tfv = TfidfVectorizer(min_df=1)\n",
    "    df = df[[\"phone_brand\", \"device_model\", \"app_lab\"]].astype(np.str).apply(\n",
    "        lambda x: \" \".join(s for s in x), axis=1).fillna(\"Missing\")\n",
    "    df_tfv = tfv.fit_transform(df)\n",
    "\n",
    "    train = df_tfv[:split_len, :]\n",
    "    test = df_tfv[split_len:, :]\n",
    "    return train, test\n",
    "\n",
    "def get_hash_data2(train, test):\n",
    "    df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "    split_len = len(train)\n",
    "\n",
    "    # TF-IDF Feature\n",
    "    tfv = TfidfVectorizer(min_df=1)\n",
    "    df = df[[\"phone_brand\", \"device_model\"]].astype(np.str).apply(\n",
    "        lambda x: \" \".join(s for s in x), axis=1).fillna(\"Missing\")\n",
    "    df_tfv = tfv.fit_transform(df)\n",
    "\n",
    "    train = df_tfv[:split_len, :]\n",
    "    test = df_tfv[split_len:, :]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainrow = np.arange(gatrain.shape[0])\n",
    "testrow = np.arange(gatest.shape[0])\n",
    "superrow= np.arange(gatrain.shape[0]+ gatest.shape[0])\n",
    "\n",
    "\n",
    "#bags for all data\n",
    "train_bag, test_bag = get_hash_data(gatrain,gatest)\n",
    "\n",
    "#bags only brand and model:\n",
    "train_bag2, test_bag2 = get_hash_data2(gatrain,gatest)\n",
    "\n",
    "\n",
    "del gatrain\n",
    "del gatest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = hstack((Xtrain, Xtr_brand_freq, Xtr_model_freq,Xtr_eventsize,Xtr_event_on_hourofday,\n",
    "                 train_bag), format='csr')\n",
    "Xtest =  hstack((Xtest, Xte_brand_freq, Xte_model_freq,Xte_eventsize,Xte_event_on_hourofday,\n",
    "                 test_bag,), format='csr')\n",
    "Xtrain_bm=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtrain_all_brand_model.npz')) \n",
    "Xtest_bm=scipy.sparse.load_npz(os.path.join(RAW_DATA_DIR,'Xtest_all_brand_model.npz'))\n",
    "\n",
    "Xtrain_ne = hstack((Xtrain_bm,Xtr_brand_freq, Xtr_model_freq, train_bag2), format='csr')\n",
    "Xtest_ne =  hstack((Xtest_bm,Xte_brand_freq, Xte_model_freq, test_bag2), format='csr')\n",
    "\n",
    "print('All features: train shape {}, test shape {}'.format(Xtrain.shape, Xtest.shape))\n",
    "\n",
    "# Reduce dimensionality\n",
    "indices = np.nonzero(Xtrain)\n",
    "columns_non_unique = indices[1]\n",
    "unique_columns = sorted(set(columns_non_unique))\n",
    "Xtrain=Xtrain.tocsc()[:,unique_columns]\n",
    "Xtest=Xtest.tocsc()[:,unique_columns]\n",
    "\n",
    "print('All features after dimensionality reduction: train shape {}, test shape {}'.format(Xtrain.shape, Xtest.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetencoder = LabelEncoder().fit(gatrain.group)\n",
    "y = targetencoder.transform(gatrain.group)\n",
    "nclasses = len(targetencoder.classes_)\n",
    "\n",
    "##Keras stuff\n",
    "dummy_y = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(num_columns):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.4, input_shape=(num_columns,)))\n",
    "    model.add(Dense(75))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(50, init='normal', activation='tanh'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.20))\n",
    "\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "folds= pd.read_csv(os.path.join(RAW_DATA_DIR,'folds_10.csv')) \n",
    "pred = np.zeros((y.shape[0],nclasses*2))\n",
    "pred_test = np.zeros((gatest.shape[0],nclasses*2))\n",
    "n_folds=len(folds[\"fold\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=events.device_id.unique() \n",
    "#creating featura has events for train and test\n",
    "gatrain['has_events']=gatrain.device_id.apply(lambda x:1 if x in s else 0)  \n",
    "gatest['has_events']=gatest.device_id.apply(lambda x:1 if x in s else 0) \n",
    "gatrain=gatrain.merge(folds,on='device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "  \n",
    "    # Use of hybrid method \n",
    "    temp = set(lst2) \n",
    "    lst3 = [value for value in lst1 if value in temp] \n",
    "    return lst3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on train and test using Logistic Regression and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_id in range(1, n_folds + 1):\n",
    "    #fold_id=1\n",
    "    inTr=gatrain.index[gatrain.fold!=fold_id] \n",
    "    inTe=gatrain.index[gatrain.fold==fold_id]\n",
    "\n",
    "    # With no events\n",
    "    train_id_ne = intersection(list(inTr), list(gatrain.index[gatrain[\"has_events\"]==0]))\n",
    "    valid_id_ne = intersection(list(inTe),list(gatrain.index[gatrain[\"has_events\"]==0]))\n",
    "    test_id_ne = list(gatest.index[gatest['has_events']==0])\n",
    "\n",
    "    # With events: Training using only common features\n",
    "    train_id_we = intersection(list(inTr), list(gatrain.index[gatrain[\"has_events\"]==1]))\n",
    "    valid_id_we = intersection(list(inTe), list(gatrain.index[gatrain[\"has_events\"]==1]))\n",
    "    test_id_we = gatest.index[gatest['has_events']==1]\n",
    "\n",
    "    # First, train on all data, but only no-events feature. Validate with no events:\n",
    "    Xtr, Ytr = Xtrain_ne[train_id, :], y[train_id]\n",
    "    Xva, Yva = Xtrain_ne[valid_id_ne, :], y[valid_id_ne]\n",
    "\n",
    "    # Logistic regression >\n",
    "    clf1 = LogisticRegression(C=0.06, multi_class='multinomial', solver='lbfgs')  # 2.38715733092\n",
    "    # Fitting logistic regression 1\n",
    "    clf1.fit(Xtr, Ytr)\n",
    "\n",
    "    # Predicting only in those with no events!\n",
    "    pred[valid_id_ne, 0:12] = clf1.predict_proba(Xva)\n",
    "    pred_test[test_id_ne, 0:12] = pred_test[test_id_ne, 0:12] + clf1.predict_proba(Xtest_ne[test_id_ne, :])\n",
    "\n",
    "    score_val = log_loss(Yva, pred[valid_id_ne, 0:12])\n",
    "    print(\"No-events: Logistic logloss for fold {} is {}\".format(fold_id, score_val))\n",
    "\n",
    "    # 2.- After, train only rows with events\n",
    "    Xtr, Ytr = Xtrain[train_id_we, :], y[train_id_we]\n",
    "    Xva, Yva = Xtrain[valid_id_we, :], y[valid_id_we]\n",
    "\n",
    "    clf2 = LogisticRegression(C=0.016, multi_class='multinomial', solver='lbfgs')  # 1.99914889909\n",
    "    clf2.fit(Xtr, Ytr)\n",
    "\n",
    "    # Predicting only in those with events!\n",
    "    pred[valid_id_we, 0:12] = clf2.predict_proba(Xva)\n",
    "    pred_test[test_id_we, 0:12] = pred_test[test_id_we, 0:12] + clf2.predict_proba(Xtest[test_id_we, :])\n",
    "\n",
    "    score_val = log_loss(Yva, pred[valid_id_we, 0:12])\n",
    "    print(\"With-events: Logistic logloss for fold {} is {}\".format(fold_id, score_val))\n",
    "\n",
    "    Xva, Yva = Xtrain[valid_id, :], y[valid_id]\n",
    "    score_val = log_loss(Yva, pred[valid_id, 0:12])\n",
    "    print(\"Total: Logistic logloss for fold {} is {}\".format(fold_id, score_val))\n",
    "\n",
    "    ## Fitting Keras! ------------------------------------------------------------------>\n",
    "    # First, train on all data, but only no-events feature. Validate with no events:\n",
    "    Xtr, Ytr_dum = Xtrain_ne[train_id, :], dummy_y[train_id]\n",
    "    Xva, Yva_dum = Xtrain_ne[valid_id_ne, :], dummy_y[valid_id_ne]\n",
    "\n",
    "    model = baseline_model(Xtr.shape[1])\n",
    "    fit = model.fit_generator(generator=batch_generator(Xtr, Ytr_dum, 381, True),\n",
    "                              nb_epoch=20,\n",
    "                              steps_per_epoch=Xtr.shape[0]/381, verbose=2,\n",
    "                              validation_data=(Xva.todense(), Yva_dum)\n",
    "                              )\n",
    "\n",
    "    # evaluate the model\n",
    "    pred[valid_id_ne, 12:25] = model.predict_generator(generator=batch_generatorp(Xva, 400, False),\n",
    "                                                       steps=Xva.shape[0]/400)\n",
    "    pred_test[test_id_ne, 12:25] = pred_test[test_id_ne, 12:25] + \\\n",
    "                                   model.predict_generator(\n",
    "                                       generator=batch_generatorp(Xtest_ne[test_id_ne, :], 400, False),\n",
    "                                       steps=Xtest_ne[test_id_ne, :].shape[0]/400)\n",
    "\n",
    "    # 2.- After, train all data (keras)\n",
    "    Xtr, Ytr_dum = Xtrain[train_id, :], dummy_y[train_id]\n",
    "    Xva, Yva_dum = Xtrain[valid_id_we, :], dummy_y[valid_id_we]\n",
    "\n",
    "    model = baseline_model(Xtr.shape[1])\n",
    "    fit = model.fit_generator(generator=batch_generator(Xtr, Ytr_dum, 381, True),\n",
    "                              nb_epoch=20,\n",
    "                              steps_per_epoch=Xtr.shape[0]/381, verbose=2,\n",
    "                              validation_data=(Xva.todense(), Yva_dum)\n",
    "                              )\n",
    "\n",
    "    # evaluate the model, and predict only with events:\n",
    "    pred[valid_id_we, 12:25] = model.predict_generator(generator=batch_generatorp(Xva, 400, False),\n",
    "                                                       steps=Xva.shape[0]/400)\n",
    "    pred_test[test_id_we, 12:25] = pred_test[test_id_we, 12:25] + \\\n",
    "                                   model.predict_generator(generator=batch_generatorp(Xtest[test_id_we, :], 400, False),\n",
    "                                                           steps=Xtest[test_id_we, :].shape[0]/400)\n",
    "\n",
    "    # pred_test[test_id_ne,0:12] = pred_test[test_id_ne,0:12] + clf1.predict_proba(Xtest_ne[test_id_ne, :])\n",
    "\n",
    "    Xva, Yva = Xtrain[valid_id, :], y[valid_id]\n",
    "    score_val = log_loss(Yva, pred[valid_id, 12:25])\n",
    "    print(\"Total: Keras logloss for fold {} is {}\".format(fold_id, score_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Averaging predictions for all folds in the test set\n",
    "pred_test /= float(n_folds) \n",
    "pred_test_mix=pred_test[:,12:25]\n",
    "pred_test_mix[test_id_ne,0:12]=pred_test[test_id_ne,0:12]\n",
    "pred_test_keras4=pd.DataFrame(pred_test_mix,index=gatest.device_id) \n",
    "\n",
    "pred_test_keras4.to_csv('pred_test_cv10_keras4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "pred_mix=pred[:,12:25]\n",
    "pred_mix[train_id_ne,0:12]=pred[train_id_ne,0:12]\n",
    "pred_train_keras4=pd.DataFrame(pred_mix,index=gatrain.device_id) \n",
    "\n",
    "pred_train_keras4.to_csv('pred_train_cv10_keras4.csv')\n",
    "pred_test_keras4.to_csv('pred_test_cv10_keras4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "pred_test_keras4.columns=['device_id']+list(targetencoder.classes_)\n",
    "pred_test_keras4.to_csv('pred_test_cv10_keras4.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Documents/input/keras4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
